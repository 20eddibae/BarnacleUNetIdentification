{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a29230-7632-45f6-98bf-3a635d0b9302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: numpy in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: Pillow in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (9.3.0)\n",
      "Requirement already satisfied: tqdm in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (4.64.1)\n",
      "Requirement already satisfied: opencv-python in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (4.8.0.76)\n",
      "Requirement already satisfied: torch in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: albumentations in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: sympy in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: scipy in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from albumentations) (1.10.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from albumentations) (0.21.0)\n",
      "Requirement already satisfied: PyYAML in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations) (1.3.2)\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/eddiebae/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy Pillow tqdm opencv-python torch torchvision albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a42072f-ea46-458d-ab9d-1810d0fed824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d22f6b6-fd39-4730-9ebe-6baf9234f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fead46-14d4-4fb0-bab3-7abe9b8ca634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarnacleDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.img_files = sorted(glob(os.path.join(data_dir, 'imgs', '*.png')))\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        mask_path = img_path.replace('imgs', 'masks')\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        mask = mask / 255.0\n",
    "        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb584eb4-caf2-497f-b186-5c73a5e01f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## U-Net Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c2d95d-e5aa-4a95-9182-c4113a039d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c7b3a7-5c15-4b46-9e3e-a03c37e8e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## U-Net Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa45ff8-7255-4a2f-9284-88a379fa163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a835b0ca-8716-44d2-af8c-edd9ce5d02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088d46ca-b93f-42b6-bfdc-1e5da6975a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/eddiebae/CS/BarnacleUNetIdentification/Pytorch-UNet/notebooks\n",
      "Dataset train image files: []\n",
      "Total in dataset: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset train image files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_dataset\u001b[38;5;241m.\u001b[39mimg_files[:\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal in dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset))\n\u001b[0;32m---> 17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/utils/data/sampler.py:144\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = BarnacleDataset('../data/tiles/train', transform=train_transform)\n",
    "val_dataset = BarnacleDataset('../data/tiles/val')\n",
    "\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Dataset train image files:\", train_dataset.img_files[:3])\n",
    "print(\"Total in dataset:\", len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f249cd-806b-4092-a15d-4db1eab87341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = BarnacleDataset('../data/tiles/train', transform=train_transform)\n",
    "val_dataset = BarnacleDataset('../data/tiles/val')\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5df7e-9e5d-4a42-93c5-ce8f649061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21cb17-5498-4697-9faf-43be435451a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(pred, target):\n",
    "    pred = torch.sigmoid(pred) > 0.5\n",
    "    target = target > 0.5\n",
    "    intersection = (pred & target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    return (2 * intersection) / (union + 1e-6)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    for imgs, masks in tqdm(loader, desc='Training'):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice_score(outputs, masks).item()\n",
    "    return total_loss / len(loader), total_dice / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(loader, desc='Validation'):\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice_score(outputs, masks).item()\n",
    "    return total_loss / len(loader), total_dice / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ab319-699e-44e7-9e9e-d3bcaf416995",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0dfaa-f2af-4da2-830a-47bd31b50b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "model = UNet(n_channels=3, n_classes=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, train_dices, val_dices = [], [], [], []\n",
    "best_val_dice = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    train_loss, train_dice = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_dice = validate_epoch(model, val_loader, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_dices.append(train_dice)\n",
    "    val_dices.append(val_dice)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}')\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        torch.save(model.state_dict(), '../models/best_barnacle_unet.pth')\n",
    "        print(f'New best model saved! Dice: {val_dice:.4f}')\n",
    "    if epoch > 5 and val_dice < 0.1:\n",
    "        print('Early stopping due to poor performance')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86097f-0d66-47f3-afcd-79c2960c29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9109e-d0a6-409a-8bf0-0161136f2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_dices, label='Train Dice')\n",
    "plt.plot(val_dices, label='Val Dice')\n",
    "plt.title('Training and Validation Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f7581-7a02-40c4-a2ce-b094bfe2b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Predictions on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74268f8-b47b-4947-828f-d7e3ad56f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../models/best_barnacle_unet.pth'))\n",
    "model.eval()\n",
    "test_batch = next(iter(val_loader))\n",
    "test_imgs, test_masks = test_batch\n",
    "with torch.no_grad():\n",
    "    test_imgs = test_imgs.to(device)\n",
    "    predictions = model(test_imgs)\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "final_dice = dice_score(predictions, test_masks.to(device)).item()\n",
    "print(f'Final Dice Score: {final_dice:.4f}')\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i in range(4):\n",
    "    plt.subplot(4, 3, i*3+1)\n",
    "    plt.imshow(test_imgs[i].cpu().permute(1, 2, 0))\n",
    "    plt.title(f'Input Image {i+1}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 3, i*3+2)\n",
    "    plt.imshow(test_masks[i].squeeze().cpu(), cmap='gray')\n",
    "    plt.title(f'Ground Truth {i+1}')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 3, i*3+3)\n",
    "    plt.imshow(predictions[i].squeeze().cpu(), cmap='gray')\n",
    "    plt.title(f'Prediction {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbca42-adf2-47d9-807b-1eda83aa368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference on New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a63a33-4bf0-4017-a62a-2e88b68b73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_image(model, img_path, device, out_threshold=0.5):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        output = torch.sigmoid(output)\n",
    "        mask = (output > out_threshold).float()\n",
    "    return mask.squeeze().cpu().numpy()\n",
    "# Example usage:\n",
    "# pred_mask = predict_full_image(model, 'path/to/new_image.png', device)\n",
    "# plt.imshow(pred_mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a7885-d608-4c31-a931-f1f37393f7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
